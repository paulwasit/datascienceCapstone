
### parameters

```{r}
setwd('D:\\datascienceCapstone')
library(dplyr)

# sample size
lang <- "en_US"
sampleSize <- 10
minOccurences <- 10
```

### sampling + sentences tokenization

```{r}
source('./helpers/sampleCreator.R')
createSample(lang, sampleSize, minOccurences)
rm(createSample,createRawSample,tokenizeSent)
```

### unigrams tokenizing 
#   + cleaning (remove duplicates / identify non words as <unk>) 
#   + filtering (identify words < minOccurences as <unk>)

```{r}
source('./helpers/unigramsCreator.R')
createUnigrams(lang, sampleSize, minOccurences)
rm(createUnigrams,tokenize,cleanUnigrams,createIndex)
```

### frequency tables

```{r}
G1_list <- readRDS(paste0("./data/",lang,".fullSample",sampleSize,".G1clean.Rds"))

source('./helpers/ngramFreqCreator.R')
nGramFreq10 <- getNgramFrequencies(G1_list,minOccurences)
rm(getNgramFrequencies,getFrequencies)

saveRDS(nGramFreq10,paste0("./data/",lang,".fullSample",sampleSize,".nGramFreq",minOccurences,".Rds"))
```

### tests

```{r}
nGramFreq10 <- readRDS("./data/en_US.fullSample10.nGramFreq10.Rds")
g4 <- nGramFreq10[["4"]]
g4t <- filter(g4, grepl("^i am",c1))
g4t
```

