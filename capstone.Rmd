
### parameters

```{r}
setwd('D:\\datascienceCapstone')
library(dplyr)

# sample size
lang <- "en_US"
sampleSize <- 10
minOccurences <- 10
```

### sample operations: sampling + sentences tokenization + unigrams

```{r}
source('./helpers/sampleUnigramsCreator.R')
createSampleUnigrams(lang, sampleSize, minOccurences)
rm(createSampleUnigrams,createSample,tokenizeSent,
   sent_token_annotator,tokenize,cleanUnigrams,filterUnigrams,createIndex)
```

### frequency tables

```{r}
G1_list <- readRDS(paste0("./data/",lang,".fullSample",sampleSize,".G1clean.Rds"))

source('./helpers/ngramFreqCreator.R')
nGramFreq <- getNgramFrequencies(G1_list,minOccurences)
rm(getNgramFrequencies,getFrequencies)

saveRDS(nGramFreq,paste0("./data/",lang,".fullSample",sampleSize,".nGramFreq.Rds"))
nGramFreq <- readRDS("./data/en_US.fullSample10.nGramFreq.Rds")
ngramFreq[["1"]] <- ngramFreq[["1"]] %>% rename(c2=c1)

test <- ngramFreq[["2"]] %>% filter(c1 == "on")
head(nGramFreq[["1"]])

token <-"on"
len <- length(token)
test2 <- ngramFreq[[as.character(len+1)]] %>% 
                           filter(c1 == token) %>%
                           select(c2,score) %>%
                           mutate(ngram=len+1) %>%
                           mutate(score=score+round(log(0.4),4)) #stupid bo penalty
test2 <- test2[1:3,]
test2
```

